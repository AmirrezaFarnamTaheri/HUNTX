name: huntx

on:
  schedule:
    - cron: "0 */2 * * *"
  workflow_dispatch:
    inputs:
      max_workers:
        description: "Number of parallel ingestion workers"
        required: false
        default: "1"
      msg_fresh_hours:
        description: "Text lookback hours for first-seen sources"
        required: false
        default: "24"
      file_fresh_hours:
        description: "File/media lookback hours for first-seen sources"
        required: false
        default: "48"
      msg_subsequent_hours:
        description: "Text lookback hours on subsequent runs (0=all new since last)"
        required: false
        default: "2"
      file_subsequent_hours:
        description: "File/media lookback hours on subsequent runs (0=all new since last)"
        required: false
        default: "2"
      reset:
        description: "Factory reset before run (wipe all state/data/outputs)"
        required: false
        type: boolean
        default: false

permissions:
  contents: write

concurrency:
  group: huntx
  cancel-in-progress: false

env:
  MAX_WORKERS: ${{ github.event.inputs.max_workers || '2' }}
  MSG_FRESH_HOURS: ${{ github.event.inputs.msg_fresh_hours || '2' }}
  FILE_FRESH_HOURS: ${{ github.event.inputs.file_fresh_hours || '2' }}
  MSG_SUBSEQUENT_HOURS: ${{ github.event.inputs.msg_subsequent_hours || '2' }}
  FILE_SUBSEQUENT_HOURS: ${{ github.event.inputs.file_subsequent_hours || '2' }}
  # S3 State Storage Configuration
  S3_BUCKET: ${{ secrets.S3_BUCKET }}
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - name: Free disk space
        run: |
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache/CodeQL
          docker system prune -af 2>/dev/null || true

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('pyproject.toml') }}
          restore-keys: ${{ runner.os }}-pip-

      - name: Install
        run: |
          python -m pip install --upgrade pip
          # Retry pip install to handle transient network issues
          n=0
          until [ "$n" -ge 3 ]; do
            pip install -e . && break
            n=$((n+1))
            echo "Install failed. Retrying in $((n*5)) seconds..."
            sleep $((n*5))
          done

      - name: Prepare directories
        run: mkdir -p persist/data/{raw,output,dist,rejects,archive,state,logs}

      - name: Restore state
        run: |
          if [[ -n "$S3_BUCKET" ]]; then
            echo "Restoring state from S3 bucket: $S3_BUCKET"
            aws s3 cp s3://$S3_BUCKET/state.db persist/state.db || echo "No state.db found in S3, starting fresh."
            # Critical: Do not suppress errors here. If sync fails, we must abort to prevent wiping S3 in the persist step.
            aws s3 sync s3://$S3_BUCKET/archive persist/data/archive --quiet
          else
            echo "No S3_BUCKET secret defined. Starting fresh (stateless run)."
          fi

      - name: Factory reset (if requested)
        if: ${{ github.event.inputs.reset == 'true' }}
        run: |
          echo "=== FACTORY RESET ==="
          rm -rf persist/state.db persist/data/{raw,output,archive,dist,rejects,artifacts,state,logs}/*
          rm -rf outputs/* outputs_dev/*

          # Wipe S3 state
          if [[ -n "$S3_BUCKET" ]]; then
            aws s3 rm s3://$S3_BUCKET/state.db || true
            aws s3 rm s3://$S3_BUCKET/archive --recursive || true
            echo "Wiped S3 state"
          fi

          # Recreate READMEs
          mkdir -p outputs outputs_dev
          echo -e "# HuntX Outputs\n\nAuto-generated build output. Do not edit manually." > outputs/README.md
          echo -e "# Dev Outputs\n\nAuto-generated with 48h rolling window. Do not edit manually." > outputs_dev/README.md

          echo "Reset complete â€” all sources will be first-seen on this run."

      - name: Print Configuration
        run: |
          echo "=== Active Configuration ==="
          echo "Max Workers: $MAX_WORKERS"
          echo "Fresh Hours (Msg/File): $MSG_FRESH_HOURS / $FILE_FRESH_HOURS"
          echo "Subsequent Hours: $MSG_SUBSEQUENT_HOURS / $FILE_SUBSEQUENT_HOURS"
          echo "S3 Bucket: ${S3_BUCKET:-'Not Configured'}"
          echo "============================"

      - name: Run huntx
        env:
          TELEGRAM_TOKEN: ${{ secrets.TELEGRAM_TOKEN }}
          PUBLISH_BOT_TOKEN: ${{ secrets.PUBLISH_BOT_TOKEN }}
          TELEGRAM_API_ID: ${{ secrets.TELEGRAM_API_ID }}
          TELEGRAM_API_HASH: ${{ secrets.TELEGRAM_API_HASH }}
          TELEGRAM_USER_SESSION: ${{ secrets.TELEGRAM_USER_SESSION }}
          HUNTX_MAX_WORKERS: ${{ env.MAX_WORKERS }}
          LOG_LEVEL: INFO
        run: |
          huntx \
            --config configs/config.prod.yaml \
            --data-dir persist/data \
            --db-path persist/state.db \
            run \
            --msg-fresh-hours ${{ env.MSG_FRESH_HOURS }} \
            --file-fresh-hours ${{ env.FILE_FRESH_HOURS }} \
            --msg-subsequent-hours ${{ env.MSG_SUBSEQUENT_HOURS }} \
            --file-subsequent-hours ${{ env.FILE_SUBSEQUENT_HOURS }}

      - name: Verify & package output
        env:
          HUNTX_DATA_DIR: persist/data
        run: python scripts/verify_output.py

      - name: Upload output
        uses: actions/upload-artifact@v4
        with:
          name: huntx-output
          path: persist/data/dist/
          retention-days: 4

      - name: Upload logs
        uses: actions/upload-artifact@v4
        with:
          name: huntx-logs
          path: persist/data/logs/
          retention-days: 4

      - name: Commit outputs to main
        run: |
          git config user.name "huntx-bot"
          git config user.email "huntx-bot@users.noreply.github.com"

          git add outputs/ outputs_dev/ 2>/dev/null || true

          if git diff --cached --quiet; then
            echo "No output changes."
          else
            git commit -m "Update outputs [skip ci]"
            
            # Retry logic for outputs push
            n=0
            until [ "$n" -ge 5 ]; do
               git push origin HEAD && break
               n=$((n+1))
               echo "Push failed. Retrying in $((n*5)) seconds..."
               sleep $((n*5))
               git pull --rebase || true
            done
          fi

      - name: Persist state
        run: |
          if [[ -n "$S3_BUCKET" ]]; then
            echo "Persisting state to S3 bucket: $S3_BUCKET"
            # Upload database
            aws s3 cp persist/state.db s3://$S3_BUCKET/state.db
            # Sync archive (only uploads new/changed files)
            aws s3 sync persist/data/archive s3://$S3_BUCKET/archive --quiet --delete
          else
            echo "No S3_BUCKET secret defined. State not persisted."
          fi
